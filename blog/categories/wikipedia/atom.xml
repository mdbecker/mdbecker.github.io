<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: wikipedia | Beckerfuffle]]></title>
  <link href="http://mdbecker.github.io/blog/categories/wikipedia/atom.xml" rel="self"/>
  <link href="http://mdbecker.github.io/"/>
  <updated>2015-03-22T15:58:07-04:00</updated>
  <id>http://mdbecker.github.io/</id>
  <author>
    <name><![CDATA[Michael Becker]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Science With Python: Part 2]]></title>
    <link href="http://mdbecker.github.io/blog/2014/11/25/data-science-with-python-part-2/"/>
    <updated>2014-11-25T07:04:41-05:00</updated>
    <id>http://mdbecker.github.io/blog/2014/11/25/data-science-with-python-part-2</id>
    <content type="html"><![CDATA[<p>This is the second post in a multi-part series wherein I will explain the details surrounding the language prediction model I presented in <a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn">my Pycon 2014 talk</a>. If you make it all the way through, you will learn how to create and deploy a language prediction model of your own. Part 1 of this series can be found <a href="https://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1/">here</a>.</p>

<p><a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn"><img src="https://raw.githubusercontent.com/mdbecker/static_files/master/pycon/Becker.png" alt="Realtime predictive analytics using scikit-learn &amp; RabbitMQ" /></a><br>
<em>Realtime predictive analytics using scikit-learn &amp; RabbitMQ</em></p>

<h2>OSEMN</h2>

<p>Let&rsquo;s start off by revisiting <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">OSEMN</a>. OSEMN (pronounced <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">awesome</a>) is a typical data science process that is followed by many data scientists. OSEMN stands for <strong>Obtain</strong>, <strong>Scrub</strong>, <strong>Explore</strong>, <strong>Model</strong>, and <strong>iNterpret</strong>. As Hilary put it in <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">a blog post on the subject</a>: &ldquo;Different data scientists have different levels of expertise with each of these 5 areas, but ideally a data scientist should be at home with them all.&rdquo; As a common data science process, this is a great start, but sometimes this isn&rsquo;t enough. If you want to make your model a critical piece of your application, you must also make it accessible and performant. For this reason, I&rsquo;ll also discuss two more steps, <strong>Deploy</strong> and <strong>Scale</strong>.</p>

<h2>Explore</h2>

<p>In this post, I&rsquo;ll cover how I <strong>explored</strong> the training data for the predictive algorithm in my talk. For those who didn&rsquo;t have a chance to watch my talk, I used data from Wikipedia to train a predictive algorithm to predict the language of some text. We use this algorithm at <a href="http://aweber.jobs/">the company I work for</a> to partition user generated content for further processing and analysis.</p>

<h2>Aggregate</h2>

<p>To start off, I counted the number of times each character occurs in each language and grabbed the top 2000 from each.</p>

<h2>Pandas</h2>

<p><strong>TODO: Add a section here showing how we can run describe by average article length by language.</strong></p>

<p>Storing the data as a list of dictionaries makes it easy to load into a <a href="http://pandas.pydata.org/pandas-docs/stable/10min.html">pandas</a> <code>DataFrame</code>! You can think of a <code>DataFrame</code> as a mix between an Excel spreadsheet and a SQL database, but Python!</p>

<p>We can calculate percentages from our letter counts using the <a href="http://pandas.pydata.org/pandas-docs/stable/basics.html#descriptive-statistics">div</a> method.</p>

<p>We can add these to as new columns to our DataFrame using the join method.</p>

<p>And if you’re using Pandas in IPython-Notebook, you can easily display the contents for inspection.</p>

<h2>Visualize</h2>

<p>Alright so now let’s use 2 unsupervised machine learning techniques, to visualize our data.
There’s a lot going on here, so let’s break it down a little.</p>

<h3>KMeans</h3>

<p>First we run the KMeans clustering algorithm on our letter percentages. KMeans automatically groups our data together based on similarity.</p>

<h3>PCA</h3>

<p>Next we run an algorithm called Principal Component Analysis on our data. We’re using PCA to reduce our number of columns from 10-thousand, to 2. The reason for this will become clearer in the next slide.</p>

<h3>Plot</h3>

<p>Finally we plot the results. Since our data only has 2 dimensions, it’s easy to plot! The color of each point corresponds to its cluster number. We can see that with 4 clusters, there is a pretty clear separation of our languages.</p>

<p>One thing I noticed right away when I plotted the data like this was that there are some clear outliers. Digging into the details revealed that most of these languages use their own unique character sets.</p>

<h2>pairwise_distances</h2>

<p>We can also use the <code>pairwise_distances</code> function from scikit-learn to find the languages that are most similar. This function calculates the Euclidean distance between each pair of points. The smaller the distance, the more similar the data points are.</p>

<p>We can see that the most similar languages are in our first cluster.</p>

<p>Croatian, Serbo-Croatian and Bosnian are the most similar followed by Indonesian &amp; Malay, Danish &amp; Norwegian, English &amp; Scots.</p>

<p>Some of these really similar languages will be useful for benchmarking our language classifier later on.</p>

<h2>Seaborn</h2>

<p>Finally, Seaborn has some really awesome graphs built-in which we can also use to visualize the similarity of languages. Here we have a violin plot which is similar to a box plot except it shows the distribution in addition to the 25th, 50th, and 75th percentiles.</p>

<p>Seaborn has a ton of other awesome graphs built in! Here’s a few other examples!</p>

<h2>Conclusion</h2>

<p>So what did we learn?</p>

<ul>
<li>Aggregating our data by language made exporing it easier.</li>
<li>We can use KMeans clustering to find natural patterns in our data.</li>
<li>PCA allows us to plot data that would be hard to visualize otherwise.</li>
<li><code>pairwise_distances</code> allows us to easily idenitify the most similar (and least similar) languages.</li>
<li>Seaborn has some really shiny graphs built-in which can allow us to make pretty data visualizations.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Science With Python: Part 1]]></title>
    <link href="http://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1/"/>
    <updated>2014-07-30T18:12:00-04:00</updated>
    <id>http://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1</id>
    <content type="html"><![CDATA[<p>This is the first post in a multi-part series wherein I will explain the details surrounding the language prediction model I presented in <a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn">my Pycon 2014 talk</a>. If you make it all the way through, you will learn how to create and deploy a language prediction model of your own.</p>

<p><a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn"><img src="https://raw.githubusercontent.com/mdbecker/static_files/master/pycon/Becker.png" alt="Realtime predictive analytics using scikit-learn &amp; RabbitMQ" /></a><br>
<em>Realtime predictive analytics using scikit-learn &amp; RabbitMQ</em></p>

<h2>OSEMN</h2>

<p>I&rsquo;m not sure if <a href="http://www.hilarymason.com/">Hilary Mason</a> originally coined the term OSEMN, but I certainly learned it from her. OSEMN (pronounced <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">awesome</a>) is a typical data science process that is followed by many data scientists. OSEMN stands for <strong>Obtain</strong>, <strong>Scrub</strong>, <strong>Explore</strong>, <strong>Model</strong>, and <strong>iNterpret</strong>. As Hilary put it in <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">a blog post on the subject</a>: &ldquo;Different data scientists have different levels of expertise with each of these 5 areas, but ideally a data scientist should be at home with them all.&rdquo; As a common data science process, this is a great start, but sometimes this isn&rsquo;t enough. If you want to make your model a critical piece of your application, you must also make it accessible and performant. For this reason, I&rsquo;ll also discuss two more steps, <strong>Deploy</strong> and <strong>Scale</strong>.</p>

<h2>Obtain &amp; Scrub</h2>

<p>In this post, I&rsquo;ll cover how I <strong>obtained</strong> and <strong>scrubbed</strong> the training data for the predictive algorithm in my talk. For those who didn&rsquo;t have a chance to watch my talk, I used data from Wikipedia to train a predictive algorithm to predict the language of some text. We use this algorithm at <a href="http://aweber.jobs/">the company I work for</a> to partition user generated content for further processing and analysis.</p>

<h3>Pagecounts</h3>

<p>So step 1 is <strong>obtaining</strong> a dataset we can use to train a predictive model. <a href="https://www.cs.drexel.edu/~urlass/">My friend Rob</a> recommended I use Wikipedia for this, so I decided to try it out. There are <a href="https://meta.wikimedia.org/wiki/Datasets">a few datasets</a> extracted from Wikipedia obtainable online at the time of this writing. Otherwise you need to generate the dataset yourself, which is what I did. I grabbed hourly page views per article for the past 5 months from <a href="http://dumps.wikimedia.org/other/pagecounts-ez/">dumps.wikimedia.org</a>. I wrote some Python scripts to aggregate these counts and dump the top 50,000 articles from each language.</p>

<h3>Export bot</h3>

<p>After this, I wrote an insanely simple bot to execute queries against the Wikipedia <a href="https://en.wikipedia.org/wiki/Special:Export"><code>Special:Export</code></a> page. Originally, I was considering using <a href="http://scrapy.org/">scrapy</a> for this since I&rsquo;ve been looking for an excuse to use it. A quick read through of the tutorial left me feeling like scrapy was overkill for my problem. I decided a simple bot would be more appropriate. I was inspecting the fields of the web-form for the <code>Special:Export</code> page using <a href="https://developer.chrome.com/devtools/index">Chrome Developer Tools</a> when I stumbled upon a pretty cool trick. Under the &ldquo;Network&rdquo; tab, if you <code>ctrl</code> click on a request, you can use &ldquo;<a href="https://developer.chrome.com/devtools/docs/network#copying-requests-as-curl-commands">Copy as cURL</a>&rdquo; to get a curl command that will reproduce the exact request made by the Chrome browser (headers, User-Agent and all). This makes it easy to write a simple bot that just interacts with a single web-form. The bot code looks a little something like this:</p>

<pre><code class="python">from subprocess import call
from urllib.parse import urlencode

curl = """curl 'https://{2}.wikipedia.org/w/index.php?title=Special:Export&amp;action=submit' -H 'Origin: https://{2}.wikipedia.org' -H 'Accept-Encoding: gzip,deflate,sdch' -H 'User-Agent: Mozilla/5.0 Chrome/35.0' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' -H 'Cache-Control: max-age=0' -H 'Referer: https://{2}.wikipedia.org/wiki/Special:Export' -H 'Connection: keep-alive' -H 'DNT: 1' --compressed --data '{0}' &gt; {1}"""

data = {
    'catname': '',
    'pages': 'Main_Page\nClimatic_Research_Unit_email_controversy\nJava\nundefined',
    'curonly': '1',
    'wpDownload': '1',
}

enc_data = urlencode(data)
call(curl.format(enc_data, filename, lang), shell=True)
</code></pre>

<p>The final version of my bot splits the list of articles into small chunks since the <code>Special:Export</code> page throws 503 errors when the requests are too large.</p>

<h3>Convert to plain text</h3>

<p>The <code>Special:Export</code> page on Wikipedia returns an XML file that contains the page contents and other pieces of information. The page contents include wiki markup, which for my purposes are not useful. I needed to <strong>scrub</strong> the Wikipedia markup to convert the pages to plain text. Fortunately, I found <a href="https://github.com/bwbaugh/wikipedia-extractor">a tool that already does this</a>. There was one downside to this tool which is that it produces output in a format that looks strikingly similar to XML, but is not actually valid XML. To address this, I wrote a simple parser using <a href="https://docs.python.org/2/library/re.html#re.MatchObject.groupdict">a regex</a> that looks something like this:</p>

<pre><code class="python">import bz2
import re

article = re.compile(r'&lt;doc id="(?P&lt;id&gt;\d+)" url="(?P&lt;url&gt;[^"]+)" title="(?P&lt;title&gt;[^"]+)"&gt;\n(?P&lt;content&gt;.+)\n&lt;\/doc&gt;', re.S|re.U)

def parse(filename):
  data = ""
  with bz2.BZ2File(filename, 'r') as f:
    for line in f:
      line = line.decode('utf-8')
      data += line
      if line.count('&lt;/doc&gt;'):
        m = article.search(data)
        if m:
          yield m.groupdict()
        data = ""
</code></pre>

<p>This function will read every page in <code>filename</code> and return a dictionary with the <code>id</code> (an integer tracking the version of the page), the <code>url</code> (a permanent link to this version of the page), the <code>title</code>, and the plain text <code>content</code> of the page. Going through the file one article at a time, and using the <code>yield</code> keyword makes this function <a href="https://wiki.python.org/moin/Generators">a generator</a> which means that it will be more memory efficient.</p>

<h2>What&rsquo;s next?</h2>

<p>In my next post I will cover the <strong>explore</strong> step using some of Python&rsquo;s state-of-the-art tools for data manipulation and visualization. You&rsquo;ll also get your first taste of <a href="http://scikit-learn.org/stable/">scikit-learn</a>, my machine learning library of choice. If you have any questions or comments, please post them below!</p>
]]></content>
  </entry>
  
</feed>
