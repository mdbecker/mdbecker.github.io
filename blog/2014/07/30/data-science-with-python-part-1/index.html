
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Data Science With Python: Part 1 - Beckerfuffle</title>
  <meta name="author" content="Michael Becker">

   
  <meta name="description" content="I'm a Senior Data Scientist at Penn Medicine where I'm building machine learning systems to improve patient outcomes by providing real-time predictive applications that empower clinicians to identify at risk individuals. In my spare time I organize the DataPhilly Meetup group and despite being terrified of public speaking I present regularly at community events and conferences.">
  
  <meta name="keywords" content="">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1">
  <link href="/favicon.png" rel="icon">
  <link href='//fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
  <link href='//fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Beckerfuffle" type="application/atom+xml">
  <script src="/js/jquery.js"></script>
  <script src="/js/bootstrap-collapse.js"></script>
  <script src="/js/modernizr-2.0.js"></script>
  <script src="/js/octopress.js" type="text/javascript"></script>
  <script src="/js/application.js"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-53278003-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <div class="navbar navbar-inverse navbar-static-top">
  	<div class="navbar-inner">
  	  <div class="container">
        <a class="btn btn-navbar" data-toggle="collapse" data-target=".navbar-responsive-collapse">
          <span class="fui-menu-24"></span>
        </a>
  	  	<div class="nav-collapse collapse navbar-responsive-collapse" style="height:0;">
  	      <ul class="nav">
<div class="AW-Form-1479709334"></div>
<script type="text/javascript">(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//forms.aweber.com/form/34/1479709334.js";
    fjs.parentNode.insertBefore(js, fjs);
    }(document, "script", "aweber-wjs-j22dfsu6o"));
</script>
    
        <li ><a href="/">Blog</a></li>
    
        <li ><a href="/blog/archives">Archives</a></li>
    
        <li ><a href="/talks">Talks</a></li>
    
        <li ><a href="/about">About</a></li>
    
</ul>

<ul class="nav pull-right">
    
    <li><a href="http://github.com/mdbecker" title="Github Profile"><i class="i-gthb social-navbar"></i></a></li>
    
    
    
    <li><a href="http://linkedin.com/in/mdbecker" title="Linkedin Profile"><i class="i-lnkdn-sign social-navbar"></i></a></li>
    
    
    <li><a href="http://twitter.com/beckerfuffle" title="Twitter Profile"><i class="i-twttr-sign social-navbar"></i></a></li>
    
    
    <li><a href="http://plus.google.com/107771119496405367239" title="Google+ Profile"><i class="i-ggl-pls-sign social-navbar"></i></a></li>
    
    
    

    
</ul>

  	    </div>
  	  </div>
  	</div>
  </div>
  <div class="container" id="main">
      <div class="row-fluid">
        <div id="content">
          <div>
<article class="hentry" role="article">
  

  <header>
  <div class="jumbotron">
    Data Science With Python: Part 1
	<h5>








  


<i class="fui-calendar-24"></i> Jul 30, 2014</h5>
  </div>
</header>
  <div class="row-fluid">
    <div class="span12">
      <p>This is the first post in a multi-part series wherein I will explain the details surrounding the language prediction model I presented in <a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn">my Pycon 2014 talk</a>. If you make it all the way through, you will learn how to create and deploy a language prediction model of your own.</p>

<p><a href="http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn"><img src="https://raw.githubusercontent.com/mdbecker/static_files/master/pycon/Becker.png" alt="Realtime predictive analytics using scikit-learn &amp; RabbitMQ" /></a><br>
<em>Realtime predictive analytics using scikit-learn &amp; RabbitMQ</em></p>

<h2>OSEMN</h2>

<p>I&rsquo;m not sure if <a href="http://www.hilarymason.com/">Hilary Mason</a> originally coined the term OSEMN, but I certainly learned it from her. OSEMN (pronounced <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">awesome</a>) is a typical data science process that is followed by many data scientists. OSEMN stands for <strong>Obtain</strong>, <strong>Scrub</strong>, <strong>Explore</strong>, <strong>Model</strong>, and <strong>iNterpret</strong>. As Hilary put it in <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">a blog post on the subject</a>: &ldquo;Different data scientists have different levels of expertise with each of these 5 areas, but ideally a data scientist should be at home with them all.&rdquo; As a common data science process, this is a great start, but sometimes this isn&rsquo;t enough. If you want to make your model a critical piece of your application, you must also make it accessible and performant. For this reason, I&rsquo;ll also discuss two more steps, <strong>Deploy</strong> and <strong>Scale</strong>.</p>

<h2>Obtain &amp; Scrub</h2>

<p>In this post, I&rsquo;ll cover how I <strong>obtained</strong> and <strong>scrubbed</strong> the training data for the predictive algorithm in my talk. For those who didn&rsquo;t have a chance to watch my talk, I used data from Wikipedia to train a predictive algorithm to predict the language of some text. We use this algorithm at <a href="http://aweber.jobs/">the company I work for</a> to partition user generated content for further processing and analysis.</p>

<h3>Pagecounts</h3>

<p>So step 1 is <strong>obtaining</strong> a dataset we can use to train a predictive model. <a href="https://www.cs.drexel.edu/~urlass/">My friend Rob</a> recommended I use Wikipedia for this, so I decided to try it out. There are <a href="https://meta.wikimedia.org/wiki/Datasets">a few datasets</a> extracted from Wikipedia obtainable online at the time of this writing. Otherwise you need to generate the dataset yourself, which is what I did. I grabbed hourly page views per article for the past 5 months from <a href="http://dumps.wikimedia.org/other/pagecounts-ez/">dumps.wikimedia.org</a>. I wrote some Python scripts to aggregate these counts and dump the top 50,000 articles from each language.</p>

<h3>Export bot</h3>

<p>After this, I wrote an insanely simple bot to execute queries against the Wikipedia <a href="https://en.wikipedia.org/wiki/Special:Export"><code>Special:Export</code></a> page. Originally, I was considering using <a href="http://scrapy.org/">scrapy</a> for this since I&rsquo;ve been looking for an excuse to use it. A quick read through of the tutorial left me feeling like scrapy was overkill for my problem. I decided a simple bot would be more appropriate. I was inspecting the fields of the web-form for the <code>Special:Export</code> page using <a href="https://developer.chrome.com/devtools/index">Chrome Developer Tools</a> when I stumbled upon a pretty cool trick. Under the &ldquo;Network&rdquo; tab, if you <code>ctrl</code> click on a request, you can use &ldquo;<a href="https://developer.chrome.com/devtools/docs/network#copying-requests-as-curl-commands">Copy as cURL</a>&rdquo; to get a curl command that will reproduce the exact request made by the Chrome browser (headers, User-Agent and all). This makes it easy to write a simple bot that just interacts with a single web-form. The bot code looks a little something like this:</p>

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">subprocess</span> <span class="kn">import</span> <span class="n">call</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urlencode</span>
</span><span class='line'>
</span><span class='line'><span class="n">curl</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;curl &#39;https://{2}.wikipedia.org/w/index.php?title=Special:Export&amp;action=submit&#39; -H &#39;Origin: https://{2}.wikipedia.org&#39; -H &#39;Accept-Encoding: gzip,deflate,sdch&#39; -H &#39;User-Agent: Mozilla/5.0 Chrome/35.0&#39; -H &#39;Content-Type: application/x-www-form-urlencoded&#39; -H &#39;Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39; -H &#39;Cache-Control: max-age=0&#39; -H &#39;Referer: https://{2}.wikipedia.org/wiki/Special:Export&#39; -H &#39;Connection: keep-alive&#39; -H &#39;DNT: 1&#39; --compressed --data &#39;{0}&#39; &gt; {1}&quot;&quot;&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>    <span class="s">&#39;catname&#39;</span><span class="p">:</span> <span class="s">&#39;&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="s">&#39;pages&#39;</span><span class="p">:</span> <span class="s">&#39;Main_Page</span><span class="se">\n</span><span class="s">Climatic_Research_Unit_email_controversy</span><span class="se">\n</span><span class="s">Java</span><span class="se">\n</span><span class="s">undefined&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="s">&#39;curonly&#39;</span><span class="p">:</span> <span class="s">&#39;1&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="s">&#39;wpDownload&#39;</span><span class="p">:</span> <span class="s">&#39;1&#39;</span><span class="p">,</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="n">enc_data</span> <span class="o">=</span> <span class="n">urlencode</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class='line'><span class="n">call</span><span class="p">(</span><span class="n">curl</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">enc_data</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">lang</span><span class="p">),</span> <span class="n">shell</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>


<p>The final version of my bot splits the list of articles into small chunks since the <code>Special:Export</code> page throws 503 errors when the requests are too large.</p>

<h3>Convert to plain text</h3>

<p>The <code>Special:Export</code> page on Wikipedia returns an XML file that contains the page contents and other pieces of information. The page contents include wiki markup, which for my purposes are not useful. I needed to <strong>scrub</strong> the Wikipedia markup to convert the pages to plain text. Fortunately, I found <a href="https://github.com/bwbaugh/wikipedia-extractor">a tool that already does this</a>. There was one downside to this tool which is that it produces output in a format that looks strikingly similar to XML, but is not actually valid XML. To address this, I wrote a simple parser using <a href="https://docs.python.org/2/library/re.html#re.MatchObject.groupdict">a regex</a> that looks something like this:</p>

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">bz2</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">article</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;&lt;doc id=&quot;(?P&lt;id&gt;\d+)&quot; url=&quot;(?P&lt;url&gt;[^&quot;]+)&quot; title=&quot;(?P&lt;title&gt;[^&quot;]+)&quot;&gt;\n(?P&lt;content&gt;.+)\n&lt;\/doc&gt;&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">S</span><span class="o">|</span><span class="n">re</span><span class="o">.</span><span class="n">U</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
</span><span class='line'>  <span class="n">data</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
</span><span class='line'>  <span class="k">with</span> <span class="n">bz2</span><span class="o">.</span><span class="n">BZ2File</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
</span><span class='line'>      <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&#39;utf-8&#39;</span><span class="p">)</span>
</span><span class='line'>      <span class="n">data</span> <span class="o">+=</span> <span class="n">line</span>
</span><span class='line'>      <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s">&#39;&lt;/doc&gt;&#39;</span><span class="p">):</span>
</span><span class='line'>        <span class="n">m</span> <span class="o">=</span> <span class="n">article</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
</span><span class='line'>          <span class="k">yield</span> <span class="n">m</span><span class="o">.</span><span class="n">groupdict</span><span class="p">()</span>
</span><span class='line'>        <span class="n">data</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>


<p>This function will read every page in <code>filename</code> and return a dictionary with the <code>id</code> (an integer tracking the version of the page), the <code>url</code> (a permanent link to this version of the page), the <code>title</code>, and the plain text <code>content</code> of the page. Going through the file one article at a time, and using the <code>yield</code> keyword makes this function <a href="https://wiki.python.org/moin/Generators">a generator</a> which means that it will be more memory efficient.</p>

<h2>What&rsquo;s next?</h2>

<p>In my next post I will cover the <strong>explore</strong> step using some of Python&rsquo;s state-of-the-art tools for data manipulation and visualization. You&rsquo;ll also get your first taste of <a href="http://scikit-learn.org/stable/">scikit-learn</a>, my machine learning library of choice. If you have any questions or comments, please post them below!</p>

    </div>
  </div>



  <footer>
    <hr>
    
    <div class="row-fluid">
      
      <div class="span6">
        <p class="meta">
        
        



  <a href="/blog/categories/data-mining/"><span class="badge">data mining</span></a>

  <a href="/blog/categories/data-science/"><span class="badge">data science</span></a>

  <a href="/blog/categories/machine-learning/"><span class="badge">machine learning</span></a>

  <a href="/blog/categories/natural-language-processing/"><span class="badge">natural language processing</span></a>

  <a href="/blog/categories/predictive-analysis/"><span class="badge">predictive analysis</span></a>

  <a href="/blog/categories/pycon/"><span class="badge">pycon</span></a>

  <a href="/blog/categories/python/"><span class="badge">python</span></a>

  <a href="/blog/categories/text-processing/"><span class="badge">text processing</span></a>

  <a href="/blog/categories/wikipedia/"><span class="badge">wikipedia</span></a>




        </p>
      </div>
      
      <div class="span6 social-sharing">
        <div class="sharing">
  <div class="addthis_toolbox addthis_default_style ">
  
  <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
  
  
  <a class="addthis_button_tweet"></a>
  
  
  <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/250/addthis_widget.js#pubid="></script>
</div>

      </div>
      
      
    </div>
    
    <div class="row-fluid">
      <div class="span12">
        <p class="meta">
          
            <a class="basic-alignment left" href="/blog/2014/04/23/pycon-2014-the-long-journey-north/" title="Previous Post: PyCon 2014: The Long Journey North">&laquo; PyCon 2014: The Long Journey North</a>
          
          
            <a class="basic-alignment right" href="/blog/2014/11/24/pydata-nyc-the-really-short-version/" title="Next Post: PyData NYC: The Really Short Version">PyData NYC: The Really Short Version &raquo;</a>
          
        </p>
      </div>
    </div>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>



        </div>
      </div>
      <div class="row-fluid">
        <footer class="footer-page" role="contentinfo">
          <p>
  Copyright &copy; 2016 - Michael Becker -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span> - Theme by <a href="http://alexgaribay.com">Alex Garibay</a>
</p>


        </footer>
      </div>
  </div>
  

<script type="text/javascript">
      var disqus_shortname = 'beckerfuffle';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1/';
        var disqus_url = 'http://mdbecker.github.io/blog/2014/07/30/data-science-with-python-part-1/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
